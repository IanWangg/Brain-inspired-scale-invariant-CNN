{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain-Inspired-Scale-Invariant-CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanWangg/Brain-inspired-scale-invariant-CNN/blob/implement-multi-level-kernels/Brain_Inspired_Scale_Invariant_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJzF5aNOTdW",
        "colab_type": "text"
      },
      "source": [
        "## Code for implementing scale invariant CNNs.\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.160 %\n",
        "tensor(59497, device='cuda:0') 60000\n",
        "Test accuracy of the model: 82.702 %\n",
        "tensor(8271, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2.5].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.210 %\n",
        "tensor(59527, device='cuda:0') 60000\n",
        "Test accuracy of the model: 72.533 %\n",
        "tensor(7254, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di4TD6kURjqA",
        "colab_type": "text"
      },
      "source": [
        "### Importing required libraries and setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21j-2kB6JyD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "66fcccb3-103a-43c5-9aab-9c44153620dd"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "!pip install torchviz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.5.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3523 sha256=bd023a0b1d63ddc198913f1fe6e6d137b48432013a1be3b5c0f45b5f536f19f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndlXr-e4Rpjh",
        "colab_type": "text"
      },
      "source": [
        "### Rewriting Conv2d to implement the scale invariant convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw64jcc9SnNg",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the base class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbM5lGSOQOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.modules.utils import _single, _pair, _triple\n",
        "from torch.nn.modules.conv import *\n",
        "\n",
        "def _reverse_repeat_tuple(t, n):\n",
        "    \"\"\"Reverse the order of `t` and repeat each element for `n` times.\n",
        "    This can be used to translate padding arg used by Conv and Pooling modules\n",
        "    to the ones used by `F.pad`.\n",
        "    \"\"\"\n",
        "    return tuple(x for x in reversed(t) for _ in range(n))\n",
        "\n",
        "class _ConvNd(Module):\n",
        "\n",
        "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
        "                     'padding_mode', 'output_padding', 'in_channels',\n",
        "                     'out_channels', 'kernel_size']\n",
        "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, transposed, output_padding,\n",
        "                 groups, bias, padding_mode):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
        "        if padding_mode not in valid_padding_modes:\n",
        "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
        "                valid_padding_modes, padding_mode))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
        "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
        "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
        "        # reverse order than the dimension.\n",
        "        self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        if self.padding_mode != 'zeros':\n",
        "            s += ', padding_mode={padding_mode}'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(_ConvNd, self).__setstate__(state)\n",
        "        if not hasattr(self, 'padding_mode'):\n",
        "            self.padding_mode = 'zeros'"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0WYIq5PSpYY",
        "colab_type": "text"
      },
      "source": [
        "#### Writing out new convolutional filter. Same number of parameters but convolutions at multiple scales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNcy4bSNRm0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "import random\n",
        "class Conv2dMultiScale(_ConvNd):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1,\n",
        "                 bias=True, padding_mode='zeros', num_scales=2, pooling_mode='max', level = 1):\n",
        "        cur_size = kernel_size\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(Conv2dMultiScale, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias, padding_mode)\n",
        "        # self.scale = nn.UpsamplingBilinear2d(size=(8,8))\n",
        "        self.scale = []\n",
        "        for i in range(num_scales - 1):\n",
        "          self.scale.append(nn.UpsamplingBilinear2d(size=(cur_size + 2, cur_size + 2)))\n",
        "        # self.scale = nn.functional.interpolate(size=(5,5), mode='bicubic')\n",
        "        self.pooling_mode = pooling_mode\n",
        "\n",
        "    def _conv_forward(self, input, weight_para):\n",
        "        out1 = F.conv2d(input, weight_para, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "        for scale in self.scale:          \n",
        "          weight = scale(weight_para)\n",
        "          padding = tuple(x+1 for x in self.padding)\n",
        "          out2 = F.conv2d(input, weight, self.bias, self.stride,\n",
        "                        padding, self.dilation, self.groups)\n",
        "          out2 = F.interpolate(out2, size=out1.shape[2:], mode='bilinear', align_corners=False)         \n",
        "          if (self.pooling_mode == 'avg'):\n",
        "            out1 = (out1 + out2)\n",
        "          elif (self.pooling_mode == 'max'):\n",
        "            out1 = torch.max(out1, out2)\n",
        "          else:\n",
        "            out1 = torch.cat([out1, out2], dim=1)      \n",
        "        return out1\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(input, self.weight)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7NbCG3hpTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b9d558ba-cb55-43ae-d2c1-9a3951a02124"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpYqRAnM8Aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "08942a56-84b5-41c7-ac87-cb2f6b2dfd0f"
      },
      "source": [
        "# Understanding sizes of CNNs, ensuring it works\n",
        "kernelSize = 3\n",
        "m = Conv2dMultiScale(64, 64, kernelSize, stride=1, padding=0, num_scales=3, pooling_mode='cat', increment=1)\n",
        "input = torch.randn(20, 64, 32, 32)\n",
        "print(input.shape)\n",
        "output = m(input)\n",
        "print(output.shape)\n",
        "output = F.max_pool2d(output, 2)\n",
        "print(output.shape)\n",
        "m2 = Conv2dMultiScale(192, 64, kernelSize, stride=1, padding=0, num_scales=3)\n",
        "output = m2(output)\n",
        "print(output.shape)\n",
        "output = F.max_pool2d(output, 2)\n",
        "print(output.shape)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 64, 32, 32])\n",
            "torch.Size([20, 192, 30, 30])\n",
            "torch.Size([20, 192, 15, 15])\n",
            "torch.Size([20, 64, 13, 13])\n",
            "torch.Size([20, 64, 6, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO3Ks9GN1Tef",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emawq_CRVXu",
        "colab_type": "text"
      },
      "source": [
        "###MNIST Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIJal8URUUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "391248d2-6e76-4921-f9bc-674e95bc5bcf"
      },
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.RandomAffine(degrees=0, scale=(1,1)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "train_data = dsets. CIFAR10(root = './data', train = True,\n",
        "                        transform = train_transform, download = True)\n",
        "\n",
        "# test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "#                        transform = test_transform)\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "# test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "#                                       batch_size = batch_size, \n",
        "#                                       shuffle = False)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sls9YItQaOm",
        "colab_type": "text"
      },
      "source": [
        "###Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJAYjO4XQf5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, lr=0.001, num_epochs=10, batch_size=64):  \n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i ,(images,labels) in enumerate(train_gen):\n",
        "      if torch.cuda.is_available():\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(images)\n",
        "      loss = loss_function(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (i+1) % 100 == 0:\n",
        "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "def test(net):  \n",
        "  if(net.multiScale):\n",
        "    print('RESULTS OF MULTISCALE CNN')\n",
        "  else:\n",
        "    print('RESULTS OF STANDARD CNN')\n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # loss_function = nn.CrossEntropyLoss()\n",
        "  for images,labels in train_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  train_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Train accuracy of the model: %.3f %%' %(train_acc))\n",
        "  print(correct, total)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images,labels in test_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  test_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Test accuracy of the model: %.3f %%' %(test_acc))\n",
        "  print(correct, total)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhSgsk9CRcW6",
        "colab_type": "text"
      },
      "source": [
        "###Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WykbLSM74b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, multiScale=True, cat = False, level_1=3, level_2=2):\n",
        "        super(Net, self).__init__()\n",
        "        self.multiScale = multiScale\n",
        "        self.cat = cat\n",
        "        self.level_1 = level_1\n",
        "        self.level_2 = level_2\n",
        "        if(self.multiScale and not cat):\n",
        "          self.conv1 = Conv2dMultiScale(3, 64, 5, 1, pooling_mode='max',num_scales=level_1)\n",
        "          self.conv2 = Conv2dMultiScale(64, 64, 5, 1, pooling_mode='max',num_scales=level_2)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        elif(cat):\n",
        "          self.conv1 = Conv2dMultiScale(3, 64, 5, 1, pooling_mode='cat',num_scales=level_1)\n",
        "          self.conv2 = Conv2dMultiScale(64 * level_1, 64 * level_1, 5, 1, pooling_mode='cat',num_scales=level_2)\n",
        "          self.fc = nn.Linear(64 * 5 * 5 * level_2 * level_1, 384)\n",
        "        else:\n",
        "          self.conv1 = nn.Conv2d(3, 64, 5, 1)\n",
        "          self.conv2 = nn.Conv2d(64, 64, 5, 1)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc1 = nn.Linear(384, 192)\n",
        "        self.fc2 = nn.Linear(192, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)        \n",
        "        x = F.relu(x)    \n",
        "        x = F.max_pool2d(x, 2)  \n",
        "        x = self.conv2(x)       \n",
        "        x = F.relu(x)      \n",
        "        x = F.max_pool2d(x, 2)     \n",
        "        x = torch.flatten(x, 1)       \n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)      \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        # else: \n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net(multiScale=True, cat=False)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKIwl6b3RffA",
        "colab_type": "text"
      },
      "source": [
        "###Train, Test and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp690s02VZM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff72717e-89c7-4444-b795-15dd3b3fe30c"
      },
      "source": [
        "train(net, lr=0.001, num_epochs=20)\n",
        "train(net, lr=0.0001, num_epochs=10)\n",
        "# train(net, lr=0.0001, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [100/781], Loss: 1.7577\n",
            "Epoch [1/20], Step [200/781], Loss: 1.7428\n",
            "Epoch [1/20], Step [300/781], Loss: 1.3531\n",
            "Epoch [1/20], Step [400/781], Loss: 1.4646\n",
            "Epoch [1/20], Step [500/781], Loss: 1.4988\n",
            "Epoch [1/20], Step [600/781], Loss: 1.3363\n",
            "Epoch [1/20], Step [700/781], Loss: 0.9909\n",
            "Epoch [2/20], Step [100/781], Loss: 1.3882\n",
            "Epoch [2/20], Step [200/781], Loss: 1.4688\n",
            "Epoch [2/20], Step [300/781], Loss: 1.3062\n",
            "Epoch [2/20], Step [400/781], Loss: 1.1546\n",
            "Epoch [2/20], Step [500/781], Loss: 1.2257\n",
            "Epoch [2/20], Step [600/781], Loss: 1.1233\n",
            "Epoch [2/20], Step [700/781], Loss: 1.1089\n",
            "Epoch [3/20], Step [100/781], Loss: 1.1396\n",
            "Epoch [3/20], Step [200/781], Loss: 0.9829\n",
            "Epoch [3/20], Step [300/781], Loss: 1.0374\n",
            "Epoch [3/20], Step [400/781], Loss: 0.8592\n",
            "Epoch [3/20], Step [500/781], Loss: 1.0205\n",
            "Epoch [3/20], Step [600/781], Loss: 1.0682\n",
            "Epoch [3/20], Step [700/781], Loss: 0.9312\n",
            "Epoch [4/20], Step [100/781], Loss: 0.9717\n",
            "Epoch [4/20], Step [200/781], Loss: 0.8950\n",
            "Epoch [4/20], Step [300/781], Loss: 1.0088\n",
            "Epoch [4/20], Step [400/781], Loss: 0.9802\n",
            "Epoch [4/20], Step [500/781], Loss: 1.2989\n",
            "Epoch [4/20], Step [600/781], Loss: 0.7847\n",
            "Epoch [4/20], Step [700/781], Loss: 0.9605\n",
            "Epoch [5/20], Step [100/781], Loss: 0.8812\n",
            "Epoch [5/20], Step [200/781], Loss: 1.0699\n",
            "Epoch [5/20], Step [300/781], Loss: 0.7329\n",
            "Epoch [5/20], Step [400/781], Loss: 0.9531\n",
            "Epoch [5/20], Step [500/781], Loss: 0.8054\n",
            "Epoch [5/20], Step [600/781], Loss: 0.9219\n",
            "Epoch [5/20], Step [700/781], Loss: 0.9007\n",
            "Epoch [6/20], Step [100/781], Loss: 0.8819\n",
            "Epoch [6/20], Step [200/781], Loss: 0.7013\n",
            "Epoch [6/20], Step [300/781], Loss: 0.7162\n",
            "Epoch [6/20], Step [400/781], Loss: 1.0610\n",
            "Epoch [6/20], Step [500/781], Loss: 1.0317\n",
            "Epoch [6/20], Step [600/781], Loss: 0.6714\n",
            "Epoch [6/20], Step [700/781], Loss: 0.8205\n",
            "Epoch [7/20], Step [100/781], Loss: 0.6103\n",
            "Epoch [7/20], Step [200/781], Loss: 0.6065\n",
            "Epoch [7/20], Step [300/781], Loss: 0.6671\n",
            "Epoch [7/20], Step [400/781], Loss: 0.5875\n",
            "Epoch [7/20], Step [500/781], Loss: 0.6042\n",
            "Epoch [7/20], Step [600/781], Loss: 0.8076\n",
            "Epoch [7/20], Step [700/781], Loss: 0.7785\n",
            "Epoch [8/20], Step [100/781], Loss: 0.6548\n",
            "Epoch [8/20], Step [200/781], Loss: 0.6686\n",
            "Epoch [8/20], Step [300/781], Loss: 0.6864\n",
            "Epoch [8/20], Step [400/781], Loss: 0.8848\n",
            "Epoch [8/20], Step [500/781], Loss: 1.0590\n",
            "Epoch [8/20], Step [600/781], Loss: 0.8185\n",
            "Epoch [8/20], Step [700/781], Loss: 0.8490\n",
            "Epoch [9/20], Step [100/781], Loss: 0.5462\n",
            "Epoch [9/20], Step [200/781], Loss: 0.6245\n",
            "Epoch [9/20], Step [300/781], Loss: 0.6415\n",
            "Epoch [9/20], Step [400/781], Loss: 0.6954\n",
            "Epoch [9/20], Step [500/781], Loss: 0.4234\n",
            "Epoch [9/20], Step [600/781], Loss: 0.5353\n",
            "Epoch [9/20], Step [700/781], Loss: 0.5513\n",
            "Epoch [10/20], Step [100/781], Loss: 0.3543\n",
            "Epoch [10/20], Step [200/781], Loss: 0.5086\n",
            "Epoch [10/20], Step [300/781], Loss: 0.4540\n",
            "Epoch [10/20], Step [400/781], Loss: 0.3858\n",
            "Epoch [10/20], Step [500/781], Loss: 0.5806\n",
            "Epoch [10/20], Step [600/781], Loss: 0.6045\n",
            "Epoch [10/20], Step [700/781], Loss: 0.3885\n",
            "Epoch [11/20], Step [100/781], Loss: 0.4725\n",
            "Epoch [11/20], Step [200/781], Loss: 0.4426\n",
            "Epoch [11/20], Step [300/781], Loss: 0.3962\n",
            "Epoch [11/20], Step [400/781], Loss: 0.8413\n",
            "Epoch [11/20], Step [500/781], Loss: 0.8390\n",
            "Epoch [11/20], Step [600/781], Loss: 0.5167\n",
            "Epoch [11/20], Step [700/781], Loss: 0.4238\n",
            "Epoch [12/20], Step [100/781], Loss: 0.4930\n",
            "Epoch [12/20], Step [200/781], Loss: 0.5852\n",
            "Epoch [12/20], Step [300/781], Loss: 0.3356\n",
            "Epoch [12/20], Step [400/781], Loss: 0.2998\n",
            "Epoch [12/20], Step [500/781], Loss: 0.4416\n",
            "Epoch [12/20], Step [600/781], Loss: 0.3737\n",
            "Epoch [12/20], Step [700/781], Loss: 0.3546\n",
            "Epoch [13/20], Step [100/781], Loss: 0.2736\n",
            "Epoch [13/20], Step [200/781], Loss: 0.3115\n",
            "Epoch [13/20], Step [300/781], Loss: 0.2907\n",
            "Epoch [13/20], Step [400/781], Loss: 0.3980\n",
            "Epoch [13/20], Step [500/781], Loss: 0.4208\n",
            "Epoch [13/20], Step [600/781], Loss: 0.2937\n",
            "Epoch [13/20], Step [700/781], Loss: 0.3890\n",
            "Epoch [14/20], Step [100/781], Loss: 0.1957\n",
            "Epoch [14/20], Step [200/781], Loss: 0.1637\n",
            "Epoch [14/20], Step [300/781], Loss: 0.3782\n",
            "Epoch [14/20], Step [400/781], Loss: 0.3275\n",
            "Epoch [14/20], Step [500/781], Loss: 0.4228\n",
            "Epoch [14/20], Step [600/781], Loss: 0.3450\n",
            "Epoch [14/20], Step [700/781], Loss: 0.3109\n",
            "Epoch [15/20], Step [100/781], Loss: 0.1362\n",
            "Epoch [15/20], Step [200/781], Loss: 0.1477\n",
            "Epoch [15/20], Step [300/781], Loss: 0.3222\n",
            "Epoch [15/20], Step [400/781], Loss: 0.2470\n",
            "Epoch [15/20], Step [500/781], Loss: 0.5275\n",
            "Epoch [15/20], Step [600/781], Loss: 0.2354\n",
            "Epoch [15/20], Step [700/781], Loss: 0.2140\n",
            "Epoch [16/20], Step [100/781], Loss: 0.2032\n",
            "Epoch [16/20], Step [200/781], Loss: 0.3192\n",
            "Epoch [16/20], Step [300/781], Loss: 0.2640\n",
            "Epoch [16/20], Step [400/781], Loss: 0.2180\n",
            "Epoch [16/20], Step [500/781], Loss: 0.2460\n",
            "Epoch [16/20], Step [600/781], Loss: 0.2927\n",
            "Epoch [16/20], Step [700/781], Loss: 0.3690\n",
            "Epoch [17/20], Step [100/781], Loss: 0.1145\n",
            "Epoch [17/20], Step [200/781], Loss: 0.1349\n",
            "Epoch [17/20], Step [300/781], Loss: 0.2364\n",
            "Epoch [17/20], Step [400/781], Loss: 0.2780\n",
            "Epoch [17/20], Step [500/781], Loss: 0.1936\n",
            "Epoch [17/20], Step [600/781], Loss: 0.2502\n",
            "Epoch [17/20], Step [700/781], Loss: 0.3756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkmy6hdofGLt",
        "colab_type": "text"
      },
      "source": [
        "###Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--PsvCNFe9Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(28),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4ZFNmftV5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6965d0b7-e6be-4790-8191-80729d072906"
      },
      "source": [
        "test(net)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 99.766 %\n",
            "tensor(49884, device='cuda:0') 50000\n",
            "Test accuracy of the model: 60.584 %\n",
            "tensor(6059, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}