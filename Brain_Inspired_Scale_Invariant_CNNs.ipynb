{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain-Inspired-Scale-Invariant-CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7aebdb43dca54fbfb821ec73e5d7c115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_30b6ae943d074ac68fc3edad70e881fd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9861e7aacada4cf3b4dd02e18f3a12a6",
              "IPY_MODEL_f95830f29a2e4fcd98f5931da793e8a3"
            ]
          }
        },
        "30b6ae943d074ac68fc3edad70e881fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9861e7aacada4cf3b4dd02e18f3a12a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_707821d1583d424d9012c6cda0a647c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc9bec28d4d04f16b43b194f160c4d4b"
          }
        },
        "f95830f29a2e4fcd98f5931da793e8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_60860dd40f79406594f6fcbe4f358dab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 31282651.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e5cfaa9f2d44acd8aab03f1184171e5"
          }
        },
        "707821d1583d424d9012c6cda0a647c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc9bec28d4d04f16b43b194f160c4d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60860dd40f79406594f6fcbe4f358dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e5cfaa9f2d44acd8aab03f1184171e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanWangg/Brain-inspired-scale-invariant-CNN/blob/implement-multi-level-kernels/Brain_Inspired_Scale_Invariant_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJzF5aNOTdW",
        "colab_type": "text"
      },
      "source": [
        "## Code for implementing scale invariant CNNs.\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.160 %\n",
        "tensor(59497, device='cuda:0') 60000\n",
        "Test accuracy of the model: 82.702 %\n",
        "tensor(8271, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2.5].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.210 %\n",
        "tensor(59527, device='cuda:0') 60000\n",
        "Test accuracy of the model: 72.533 %\n",
        "tensor(7254, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di4TD6kURjqA",
        "colab_type": "text"
      },
      "source": [
        "### Importing required libraries and setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21j-2kB6JyD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2649f79a-62b0-4278-f7aa-5db429a692b8"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 10kB 31.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.5.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3523 sha256=8c2a5f6f81ecdd8077bf6e93b5dc38025fdab610b1d8fb41e635bf5ade4e7003\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndlXr-e4Rpjh",
        "colab_type": "text"
      },
      "source": [
        "### Rewriting Conv2d to implement the scale invariant convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw64jcc9SnNg",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the base class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbM5lGSOQOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.modules.utils import _single, _pair, _triple\n",
        "from torch.nn.modules.conv import *\n",
        "\n",
        "def _reverse_repeat_tuple(t, n):\n",
        "    \"\"\"Reverse the order of `t` and repeat each element for `n` times.\n",
        "    This can be used to translate padding arg used by Conv and Pooling modules\n",
        "    to the ones used by `F.pad`.\n",
        "    \"\"\"\n",
        "    return tuple(x for x in reversed(t) for _ in range(n))\n",
        "\n",
        "class _ConvNd(Module):\n",
        "\n",
        "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
        "                     'padding_mode', 'output_padding', 'in_channels',\n",
        "                     'out_channels', 'kernel_size']\n",
        "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, transposed, output_padding,\n",
        "                 groups, bias, padding_mode):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
        "        if padding_mode not in valid_padding_modes:\n",
        "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
        "                valid_padding_modes, padding_mode))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
        "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
        "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
        "        # reverse order than the dimension.\n",
        "        self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        if self.padding_mode != 'zeros':\n",
        "            s += ', padding_mode={padding_mode}'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(_ConvNd, self).__setstate__(state)\n",
        "        if not hasattr(self, 'padding_mode'):\n",
        "            self.padding_mode = 'zeros'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0WYIq5PSpYY",
        "colab_type": "text"
      },
      "source": [
        "#### Writing out new convolutional filter. Same number of parameters but convolutions at multiple scales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNcy4bSNRm0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "import random\n",
        "class Conv2dMultiScale(_ConvNd):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1,\n",
        "                 bias=True, padding_mode='zeros', num_scales=2, pooling_mode='max', level = 1):\n",
        "        cur_size = kernel_size\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(Conv2dMultiScale, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias, padding_mode)\n",
        "        # self.scale = nn.UpsamplingBilinear2d(size=(8,8))\n",
        "        self.scale = []\n",
        "        for i in range(num_scales - 1):\n",
        "          self.scale.append(nn.UpsamplingBilinear2d(size=(cur_size + 2, cur_size + 2)))\n",
        "        # self.scale = nn.functional.interpolate(size=(5,5), mode='bicubic')\n",
        "        self.pooling_mode = pooling_mode\n",
        "\n",
        "    def _conv_forward(self, input, weight_para):\n",
        "        out1 = F.conv2d(input, weight_para, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "        for scale in self.scale:          \n",
        "          weight = scale(weight_para)\n",
        "          padding = tuple(x+1 for x in self.padding)\n",
        "          out2 = F.conv2d(input, weight, self.bias, self.stride,\n",
        "                        padding, self.dilation, self.groups)\n",
        "          out2 = F.interpolate(out2, size=out1.shape[2:], mode='bilinear', align_corners=False)         \n",
        "          if (self.pooling_mode == 'avg'):\n",
        "            out1 = (out1 + out2)\n",
        "          elif (self.pooling_mode == 'max'):\n",
        "            out1 = torch.max(out1, out2)\n",
        "          else:\n",
        "            out1 = torch.cat([out1, out2], dim=1)      \n",
        "        return out1\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(input, self.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7NbCG3hpTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d8ef955b-6140-4ce1-9f96-e9481102f64a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpYqRAnM8Aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cc2799fe-7bf7-41f8-8a09-055ad5200e73"
      },
      "source": [
        "# Understanding sizes of CNNs, ensuring it works\n",
        "kernelSize = 5\n",
        "m = Conv2dMultiScale(64, 64, kernelSize, stride=1, padding=0, num_scales=3, pooling_mode='cat')\n",
        "input = torch.randn(20, 64, 28, 28)\n",
        "print(input.shape)\n",
        "output = m(input)\n",
        "print(output.shape)\n",
        "output = F.max_pool2d(output, 2)\n",
        "print(output.shape)\n",
        "m2 = Conv2dMultiScale(192, 64, kernelSize, stride=1, padding=0, num_scales=3)\n",
        "output = m2(output)\n",
        "print(output.shape)\n",
        "output = F.max_pool2d(output, 2)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 64, 28, 28])\n",
            "torch.Size([20, 192, 24, 24])\n",
            "torch.Size([20, 192, 12, 12])\n",
            "torch.Size([20, 64, 8, 8])\n",
            "torch.Size([20, 64, 4, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO3Ks9GN1Tef",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emawq_CRVXu",
        "colab_type": "text"
      },
      "source": [
        "###MNIST Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIJal8URUUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "7aebdb43dca54fbfb821ec73e5d7c115",
            "30b6ae943d074ac68fc3edad70e881fd",
            "9861e7aacada4cf3b4dd02e18f3a12a6",
            "f95830f29a2e4fcd98f5931da793e8a3",
            "707821d1583d424d9012c6cda0a647c0",
            "bc9bec28d4d04f16b43b194f160c4d4b",
            "60860dd40f79406594f6fcbe4f358dab",
            "0e5cfaa9f2d44acd8aab03f1184171e5"
          ]
        },
        "outputId": "a424e165-8439-407c-a723-9e106c8057f9"
      },
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.RandomAffine(degrees=0, scale=(1,1)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "train_data = dsets.CIFAR10(root = './data', train = True,\n",
        "                        transform = train_transform, download = True)\n",
        "\n",
        "# test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "#                        transform = test_transform)\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "# test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "#                                       batch_size = batch_size, \n",
        "#                                       shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aebdb43dca54fbfb821ec73e5d7c115",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sls9YItQaOm",
        "colab_type": "text"
      },
      "source": [
        "###Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJAYjO4XQf5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, lr=0.001, num_epochs=10, batch_size=64):  \n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i ,(images,labels) in enumerate(train_gen):\n",
        "      if torch.cuda.is_available():\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(images)\n",
        "      loss = loss_function(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (i+1) % 100 == 0:\n",
        "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "def test(net):  \n",
        "  if(net.multiScale):\n",
        "    print('RESULTS OF MULTISCALE CNN')\n",
        "  else:\n",
        "    print('RESULTS OF STANDARD CNN')\n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # loss_function = nn.CrossEntropyLoss()\n",
        "  for images,labels in train_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  train_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Train accuracy of the model: %.3f %%' %(train_acc))\n",
        "  print(correct, total)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images,labels in test_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  test_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Test accuracy of the model: %.3f %%' %(test_acc))\n",
        "  print(correct, total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhSgsk9CRcW6",
        "colab_type": "text"
      },
      "source": [
        "###Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WykbLSM74b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, multiScale=True, cat = False, level_1=3, level_2=2):\n",
        "        super(Net, self).__init__()\n",
        "        self.multiScale = multiScale\n",
        "        self.cat = cat\n",
        "        self.level_1 = level_1\n",
        "        self.level_2 = level_2\n",
        "        if(self.multiScale and not cat):\n",
        "          self.conv1 = Conv2dMultiScale(3, 64, 5, 1, pooling_mode='max', num_scales=level_1)\n",
        "          self.conv2 = Conv2dMultiScale(64, 64, 5, 1, pooling_mode='max', num_scales=level_2)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        elif(cat and not self.multiScale):\n",
        "          self.conv1 = Conv2dMultiScale(3, 64, 5, 1, pooling_mode='cat', num_scales=level_1)\n",
        "          self.conv2 = Conv2dMultiScale(64 * level_1, 64 * level_1, 5, 1, pooling_mode='cat', num_scales=level_2)\n",
        "          self.fc = nn.Linear(64 * 5 * 5 * level_2 * level_1, 384)\n",
        "        else:\n",
        "          self.conv1 = nn.Conv2d(3, 64, 5, 1)\n",
        "          self.conv2 = nn.Conv2d(64, 64, 5, 1)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc1 = nn.Linear(384, 192)\n",
        "        self.fc2 = nn.Linear(192, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)        \n",
        "        x = F.relu(x)    \n",
        "        x = F.max_pool2d(x, 2)  \n",
        "        x = self.conv2(x)       \n",
        "        x = F.relu(x)      \n",
        "        x = F.max_pool2d(x, 2)     \n",
        "        x = torch.flatten(x, 1)       \n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)      \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        # else: \n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net(multiScale=True, cat=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKIwl6b3RffA",
        "colab_type": "text"
      },
      "source": [
        "###Train, Test and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp690s02VZM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ddbbc44-caa2-485a-ec83-d50e71c5daee"
      },
      "source": [
        "train(net, lr=0.0003, num_epochs=3)\n",
        "train(net, lr=0.001, num_epochs=10)\n",
        "train(net, lr=0.0003, num_epochs=5)\n",
        "# train(net, lr=0.001, num_epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step [100/781], Loss: 1.8365\n",
            "Epoch [1/3], Step [200/781], Loss: 1.6153\n",
            "Epoch [1/3], Step [300/781], Loss: 1.6710\n",
            "Epoch [1/3], Step [400/781], Loss: 1.5945\n",
            "Epoch [1/3], Step [500/781], Loss: 1.7932\n",
            "Epoch [1/3], Step [600/781], Loss: 1.6324\n",
            "Epoch [1/3], Step [700/781], Loss: 1.5223\n",
            "Epoch [2/3], Step [100/781], Loss: 1.3177\n",
            "Epoch [2/3], Step [200/781], Loss: 1.5321\n",
            "Epoch [2/3], Step [300/781], Loss: 1.2883\n",
            "Epoch [2/3], Step [400/781], Loss: 1.5132\n",
            "Epoch [2/3], Step [500/781], Loss: 1.2654\n",
            "Epoch [2/3], Step [600/781], Loss: 1.3236\n",
            "Epoch [2/3], Step [700/781], Loss: 1.1006\n",
            "Epoch [3/3], Step [100/781], Loss: 1.2073\n",
            "Epoch [3/3], Step [200/781], Loss: 1.1447\n",
            "Epoch [3/3], Step [300/781], Loss: 1.1815\n",
            "Epoch [3/3], Step [400/781], Loss: 1.1729\n",
            "Epoch [3/3], Step [500/781], Loss: 1.2442\n",
            "Epoch [3/3], Step [600/781], Loss: 1.0583\n",
            "Epoch [3/3], Step [700/781], Loss: 1.2643\n",
            "Epoch [1/10], Step [100/781], Loss: 1.2945\n",
            "Epoch [1/10], Step [200/781], Loss: 1.0744\n",
            "Epoch [1/10], Step [300/781], Loss: 1.1249\n",
            "Epoch [1/10], Step [400/781], Loss: 1.3231\n",
            "Epoch [1/10], Step [500/781], Loss: 1.0678\n",
            "Epoch [1/10], Step [600/781], Loss: 1.1139\n",
            "Epoch [1/10], Step [700/781], Loss: 1.0168\n",
            "Epoch [2/10], Step [100/781], Loss: 1.4423\n",
            "Epoch [2/10], Step [200/781], Loss: 0.7555\n",
            "Epoch [2/10], Step [300/781], Loss: 1.0068\n",
            "Epoch [2/10], Step [400/781], Loss: 0.9827\n",
            "Epoch [2/10], Step [500/781], Loss: 1.0072\n",
            "Epoch [2/10], Step [600/781], Loss: 0.9412\n",
            "Epoch [2/10], Step [700/781], Loss: 0.8611\n",
            "Epoch [3/10], Step [100/781], Loss: 0.9265\n",
            "Epoch [3/10], Step [200/781], Loss: 0.8286\n",
            "Epoch [3/10], Step [300/781], Loss: 0.9094\n",
            "Epoch [3/10], Step [400/781], Loss: 0.9057\n",
            "Epoch [3/10], Step [500/781], Loss: 1.0564\n",
            "Epoch [3/10], Step [600/781], Loss: 0.6873\n",
            "Epoch [3/10], Step [700/781], Loss: 1.0723\n",
            "Epoch [4/10], Step [100/781], Loss: 0.9654\n",
            "Epoch [4/10], Step [200/781], Loss: 0.7511\n",
            "Epoch [4/10], Step [300/781], Loss: 0.8694\n",
            "Epoch [4/10], Step [400/781], Loss: 0.5734\n",
            "Epoch [4/10], Step [500/781], Loss: 0.6423\n",
            "Epoch [4/10], Step [600/781], Loss: 0.5547\n",
            "Epoch [4/10], Step [700/781], Loss: 0.9191\n",
            "Epoch [5/10], Step [100/781], Loss: 0.6990\n",
            "Epoch [5/10], Step [200/781], Loss: 0.7129\n",
            "Epoch [5/10], Step [300/781], Loss: 0.7986\n",
            "Epoch [5/10], Step [400/781], Loss: 0.6189\n",
            "Epoch [5/10], Step [500/781], Loss: 0.7367\n",
            "Epoch [5/10], Step [600/781], Loss: 0.7177\n",
            "Epoch [5/10], Step [700/781], Loss: 0.5586\n",
            "Epoch [6/10], Step [100/781], Loss: 0.6094\n",
            "Epoch [6/10], Step [200/781], Loss: 0.5502\n",
            "Epoch [6/10], Step [300/781], Loss: 0.4808\n",
            "Epoch [6/10], Step [400/781], Loss: 0.6527\n",
            "Epoch [6/10], Step [500/781], Loss: 0.4889\n",
            "Epoch [6/10], Step [600/781], Loss: 0.7091\n",
            "Epoch [6/10], Step [700/781], Loss: 0.4296\n",
            "Epoch [7/10], Step [100/781], Loss: 0.3840\n",
            "Epoch [7/10], Step [200/781], Loss: 0.7140\n",
            "Epoch [7/10], Step [300/781], Loss: 0.3347\n",
            "Epoch [7/10], Step [400/781], Loss: 0.4204\n",
            "Epoch [7/10], Step [500/781], Loss: 0.3175\n",
            "Epoch [7/10], Step [600/781], Loss: 0.3474\n",
            "Epoch [7/10], Step [700/781], Loss: 0.7173\n",
            "Epoch [8/10], Step [100/781], Loss: 0.3962\n",
            "Epoch [8/10], Step [200/781], Loss: 0.3856\n",
            "Epoch [8/10], Step [300/781], Loss: 0.3126\n",
            "Epoch [8/10], Step [400/781], Loss: 0.5060\n",
            "Epoch [8/10], Step [500/781], Loss: 0.2926\n",
            "Epoch [8/10], Step [600/781], Loss: 0.4040\n",
            "Epoch [8/10], Step [700/781], Loss: 0.3167\n",
            "Epoch [9/10], Step [100/781], Loss: 0.3946\n",
            "Epoch [9/10], Step [200/781], Loss: 0.3761\n",
            "Epoch [9/10], Step [300/781], Loss: 0.4922\n",
            "Epoch [9/10], Step [400/781], Loss: 0.3473\n",
            "Epoch [9/10], Step [500/781], Loss: 0.4776\n",
            "Epoch [9/10], Step [600/781], Loss: 0.3950\n",
            "Epoch [9/10], Step [700/781], Loss: 0.4327\n",
            "Epoch [10/10], Step [100/781], Loss: 0.2008\n",
            "Epoch [10/10], Step [200/781], Loss: 0.2821\n",
            "Epoch [10/10], Step [300/781], Loss: 0.2319\n",
            "Epoch [10/10], Step [400/781], Loss: 0.4213\n",
            "Epoch [10/10], Step [500/781], Loss: 0.3780\n",
            "Epoch [10/10], Step [600/781], Loss: 0.2164\n",
            "Epoch [10/10], Step [700/781], Loss: 0.4112\n",
            "Epoch [1/5], Step [100/781], Loss: 0.0807\n",
            "Epoch [1/5], Step [200/781], Loss: 0.1505\n",
            "Epoch [1/5], Step [300/781], Loss: 0.1463\n",
            "Epoch [1/5], Step [400/781], Loss: 0.1646\n",
            "Epoch [1/5], Step [500/781], Loss: 0.1574\n",
            "Epoch [1/5], Step [600/781], Loss: 0.1017\n",
            "Epoch [1/5], Step [700/781], Loss: 0.1255\n",
            "Epoch [2/5], Step [100/781], Loss: 0.1121\n",
            "Epoch [2/5], Step [200/781], Loss: 0.0404\n",
            "Epoch [2/5], Step [300/781], Loss: 0.0531\n",
            "Epoch [2/5], Step [400/781], Loss: 0.1656\n",
            "Epoch [2/5], Step [500/781], Loss: 0.0978\n",
            "Epoch [2/5], Step [600/781], Loss: 0.0761\n",
            "Epoch [2/5], Step [700/781], Loss: 0.0490\n",
            "Epoch [3/5], Step [100/781], Loss: 0.0993\n",
            "Epoch [3/5], Step [200/781], Loss: 0.0447\n",
            "Epoch [3/5], Step [300/781], Loss: 0.0769\n",
            "Epoch [3/5], Step [400/781], Loss: 0.0296\n",
            "Epoch [3/5], Step [500/781], Loss: 0.0864\n",
            "Epoch [3/5], Step [600/781], Loss: 0.0950\n",
            "Epoch [3/5], Step [700/781], Loss: 0.1260\n",
            "Epoch [4/5], Step [100/781], Loss: 0.0725\n",
            "Epoch [4/5], Step [200/781], Loss: 0.0128\n",
            "Epoch [4/5], Step [300/781], Loss: 0.0434\n",
            "Epoch [4/5], Step [400/781], Loss: 0.0319\n",
            "Epoch [4/5], Step [500/781], Loss: 0.0135\n",
            "Epoch [4/5], Step [600/781], Loss: 0.0432\n",
            "Epoch [4/5], Step [700/781], Loss: 0.0458\n",
            "Epoch [5/5], Step [100/781], Loss: 0.1144\n",
            "Epoch [5/5], Step [200/781], Loss: 0.0137\n",
            "Epoch [5/5], Step [300/781], Loss: 0.0234\n",
            "Epoch [5/5], Step [400/781], Loss: 0.0135\n",
            "Epoch [5/5], Step [500/781], Loss: 0.0776\n",
            "Epoch [5/5], Step [600/781], Loss: 0.0426\n",
            "Epoch [5/5], Step [700/781], Loss: 0.0500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkmy6hdofGLt",
        "colab_type": "text"
      },
      "source": [
        "###Test on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtRE-fD3nNS7",
        "colab_type": "text"
      },
      "source": [
        "###32 -> 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--PsvCNFe9Xt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1c634179-25aa-42c9-ac93-6fae4b7464a9"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform, download=True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)\n",
        "print(\"Architecture:\\n\",\"num_scale:\",net.level_1,\",\",net.level_2,\"\\nnet.Multiscale=\",net.multiScale,\"\\ncat=\",net.cat)\n",
        "test(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 98.426 %\n",
            "tensor(49214, device='cuda:0') 50000\n",
            "Test accuracy of the model: 71.053 %\n",
            "tensor(7106, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNWAFnSonUOy",
        "colab_type": "text"
      },
      "source": [
        "###32 -> 28"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4ZFNmftV5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "66aae275-950f-4ec7-a04b-6fa1f424d47f"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(28),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform, download=True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)\n",
        "print(\"Architecture:\\n\",\"num_scale:\",net.level_1,\",\",net.level_2,\"\\nnet.Multiscale=\",net.multiScale,\"\\ncat=\",net.cat)\n",
        "test(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 98.426 %\n",
            "tensor(49214, device='cuda:0') 50000\n",
            "Test accuracy of the model: 65.703 %\n",
            "tensor(6571, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRyjrSxKnWYI",
        "colab_type": "text"
      },
      "source": [
        "###32 -> 24"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iafy3nlNnIkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ff3679fb-277a-4764-fb69-7beaaeea392d"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(24),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform, download=True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)\n",
        "print(\"Architecture:\\n\",\"num_scale:\",net.level_1,\",\",net.level_2,\"\\nnet.Multiscale=\",net.multiScale,\"\\ncat=\",net.cat)\n",
        "test(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 98.426 %\n",
            "tensor(49214, device='cuda:0') 50000\n",
            "Test accuracy of the model: 56.344 %\n",
            "tensor(5635, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}