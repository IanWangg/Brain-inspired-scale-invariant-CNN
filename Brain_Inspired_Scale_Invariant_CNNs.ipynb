{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain-Inspired-Scale-Invariant-CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59c6594fd4d14ff6846f880e2405e703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_216932a018204a3591b646b67a6f9b38",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_185367bf81cc47789609587c9ac58ad8",
              "IPY_MODEL_2a7471124d6849cc8c2c8058a42f83db"
            ]
          }
        },
        "216932a018204a3591b646b67a6f9b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "185367bf81cc47789609587c9ac58ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0c56a8f371d34af5895ab153a574a8e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bef8cbf7d22f4acaa5fcc48007d816cc"
          }
        },
        "2a7471124d6849cc8c2c8058a42f83db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cd48320bee34ac3b7bf2fe43c7792b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 86294980.28it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2c087f8c2604ad59d6010f381db3e91"
          }
        },
        "0c56a8f371d34af5895ab153a574a8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bef8cbf7d22f4acaa5fcc48007d816cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cd48320bee34ac3b7bf2fe43c7792b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2c087f8c2604ad59d6010f381db3e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJzF5aNOTdW",
        "colab_type": "text"
      },
      "source": [
        "## Code for implementing scale invariant CNNs.\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.160 %\n",
        "tensor(59497, device='cuda:0') 60000\n",
        "Test accuracy of the model: 82.702 %\n",
        "tensor(8271, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2.5].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.210 %\n",
        "tensor(59527, device='cuda:0') 60000\n",
        "Test accuracy of the model: 72.533 %\n",
        "tensor(7254, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di4TD6kURjqA",
        "colab_type": "text"
      },
      "source": [
        "### Importing required libraries and setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21j-2kB6JyD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a0822551-8b94-4a85-bcf4-9f4d1b93ef92"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "!pip install torchviz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.5.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndlXr-e4Rpjh",
        "colab_type": "text"
      },
      "source": [
        "### Rewriting Conv2d to implement the scale invariant convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw64jcc9SnNg",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the base class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbM5lGSOQOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.modules.utils import _single, _pair, _triple\n",
        "from torch.nn.modules.conv import *\n",
        "\n",
        "def _reverse_repeat_tuple(t, n):\n",
        "    \"\"\"Reverse the order of `t` and repeat each element for `n` times.\n",
        "    This can be used to translate padding arg used by Conv and Pooling modules\n",
        "    to the ones used by `F.pad`.\n",
        "    \"\"\"\n",
        "    return tuple(x for x in reversed(t) for _ in range(n))\n",
        "\n",
        "class _ConvNd(Module):\n",
        "\n",
        "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
        "                     'padding_mode', 'output_padding', 'in_channels',\n",
        "                     'out_channels', 'kernel_size']\n",
        "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, transposed, output_padding,\n",
        "                 groups, bias, padding_mode):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
        "        if padding_mode not in valid_padding_modes:\n",
        "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
        "                valid_padding_modes, padding_mode))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
        "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
        "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
        "        # reverse order than the dimension.\n",
        "        self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.normal_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        if self.padding_mode != 'zeros':\n",
        "            s += ', padding_mode={padding_mode}'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(_ConvNd, self).__setstate__(state)\n",
        "        if not hasattr(self, 'padding_mode'):\n",
        "            self.padding_mode = 'zeros'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0WYIq5PSpYY",
        "colab_type": "text"
      },
      "source": [
        "#### Writing out new convolutional filter. Same number of parameters but convolutions at multiple scales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNcy4bSNRm0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "import random\n",
        "\n",
        "\n",
        "class Conv2dMultiScale(_ConvNd):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1,\n",
        "                 bias=True, padding_mode='zeros', levels=2, pooling_mode='max'):\n",
        "        cur_size = kernel_size\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(Conv2dMultiScale, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias, padding_mode)\n",
        "        # self.scale = nn.UpsamplingBilinear2d(size=(8,8))\n",
        "        self.scale = []\n",
        "        for i in range(levels - 1):\n",
        "          self.scale.append(nn.UpsamplingBilinear2d(size=(cur_size + 2, cur_size + 2)))\n",
        "        # self.scale = nn.functional.interpolate(size=(5,5), mode='bicubic')\n",
        "        self.pooling_mode = pooling_mode\n",
        "\n",
        "    def _conv_forward(self, input, weight_para):\n",
        "\n",
        "        # Typically this is the only thing that done\n",
        "        out1 = F.conv2d(input, weight_para, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "        # print(type(weight_para))\n",
        "        weight = weight_para[:]\n",
        "        # print(type(weight))\n",
        "        for scale in self.scale:\n",
        "          # Upscaling the weights\n",
        "          weight = scale(weight)\n",
        "          # Adjusting padding to keep same output side\n",
        "          padding = tuple(x+1 for x in self.padding)\n",
        "          # Running convolution on the bigger scale\n",
        "          out2 = F.conv2d(input, weight, self.bias, self.stride,\n",
        "                        padding, self.dilation, self.groups)\n",
        "          #crop the output feature map\n",
        "          out2 = F.interpolate(out2, size=out1.shape[2:], mode='bilinear', align_corners=True)\n",
        "          # print(type(out1))\n",
        "          # print(type(out2))\n",
        "          if (self.pooling_mode == 'avg'):\n",
        "            out1 = out1 + out2\n",
        "          elif (self.pooling_mode == 'max'):\n",
        "            out1 = torch.max(out1, out2)\n",
        "            \n",
        "        return out1\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(input, self.weight)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7NbCG3hpTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b624672-c32c-429a-bdc7-33354d6616cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpYqRAnM8Aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6193bc66-1e09-4fd4-d6a7-de12e532c1fd"
      },
      "source": [
        "# Understanding sizes of CNNs, ensuring it works\n",
        "kernelSize = 3\n",
        "m = Conv2dMultiScale(1, 32, kernelSize, stride=1, padding=0, levels=3, pooling_mode='max')\n",
        "input = torch.randn(20, 1, 32, 32)\n",
        "print(input.shape)\n",
        "output = m(input)\n",
        "print(output.shape)\n",
        "# output = F.max_pool2d(output, 2)\n",
        "# print(output.shape)\n",
        "# m2 = Conv2dMultiScale(32, 64, kernelSize, stride=1, padding=0, levels=3)\n",
        "# output = m2(output)\n",
        "# print(output.shape)\n",
        "# output = F.max_pool2d(output, 2)\n",
        "# print(output.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 1, 32, 32])\n",
            "torch.Size([20, 32, 30, 30])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO3Ks9GN1Tef",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emawq_CRVXu",
        "colab_type": "text"
      },
      "source": [
        "###MNIST Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIJal8URUUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "59c6594fd4d14ff6846f880e2405e703",
            "216932a018204a3591b646b67a6f9b38",
            "185367bf81cc47789609587c9ac58ad8",
            "2a7471124d6849cc8c2c8058a42f83db",
            "0c56a8f371d34af5895ab153a574a8e7",
            "bef8cbf7d22f4acaa5fcc48007d816cc",
            "9cd48320bee34ac3b7bf2fe43c7792b8",
            "d2c087f8c2604ad59d6010f381db3e91"
          ]
        },
        "outputId": "7b1696b1-c0c8-4577-abf6-42edd787a741"
      },
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, scale=(1,2)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_data = dsets. CIFAR10(root = './data', train = True,\n",
        "                        transform = train_transform, download = True)\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform)\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59c6594fd4d14ff6846f880e2405e703",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sls9YItQaOm",
        "colab_type": "text"
      },
      "source": [
        "###Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJAYjO4XQf5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, lr=0.001, num_epochs=10, batch_size=64):  \n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i ,(images,labels) in enumerate(train_gen):\n",
        "      if torch.cuda.is_available():\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(images)\n",
        "      loss = loss_function(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (i+1) % 100 == 0:\n",
        "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "        \n",
        "def test(net):  \n",
        "  if(net.multiScale):\n",
        "    print('RESULTS OF MULTISCALE CNN')\n",
        "  else:\n",
        "    print('RESULTS OF STANDARD CNN')\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # loss_function = nn.CrossEntropyLoss()\n",
        "  for images,labels in train_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  train_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Train accuracy of the model: %.3f %%' %(train_acc))\n",
        "  print(correct, total)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images,labels in test_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  test_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Test accuracy of the model: %.3f %%' %(test_acc))\n",
        "  print(correct, total)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhSgsk9CRcW6",
        "colab_type": "text"
      },
      "source": [
        "###Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WykbLSM74b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, multiScale=True, cat = True, level_1=3, level_2=2):\n",
        "        super(Net, self).__init__()\n",
        "        self.multiScale = multiScale\n",
        "        self.cat = cat\n",
        "        self.level_1 = level_1\n",
        "        self.level_2 = level_2\n",
        "        if(multiScale):\n",
        "          if (not cat):\n",
        "            self.conv1 = Conv2dMultiScale(3, 64, 3, 1, pooling_mode='avg',levels=level_1)\n",
        "            self.conv2 = Conv2dMultiScale(64, 64, 3, 1, pooling_mode='avg',levels=level_2)\n",
        "            self.fc = nn.Linear(64 * 6 * 6, 384)\n",
        "          else:\n",
        "            self.conv1 = Conv2dMultiScale(3, 64, 3, 1, pooling_mode='cat',levels=level_1)\n",
        "            self.conv2 = Conv2dMultiScale(64, 64, 3, 1, pooling_mode='cat',levels=level_2)\n",
        "            self.fc = nn.Linear(64 * 6 * 6, 384)\n",
        "        else:\n",
        "          self.conv1 = nn.Conv2d(3, 64, 5, 1)\n",
        "          self.conv2 = nn.Conv2d(64, 64, 5, 1)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc1 = nn.Linear(384, 192)\n",
        "        self.fc2 = nn.Linear(192, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (not self.cat or not self.multiScale):\n",
        "          #pooling operation:\n",
        "          x = self.conv1(x)        \n",
        "          x = F.relu(x)    \n",
        "          x = F.max_pool2d(x, 2)  \n",
        "          x = self.conv2(x)       \n",
        "          x = F.relu(x)      \n",
        "          x = F.max_pool2d(x, 2)     \n",
        "          x = torch.flatten(x, 1)       \n",
        "          x = self.fc(x)\n",
        "          x = F.relu(x)      \n",
        "          x = self.fc1(x)\n",
        "          x = F.relu(x)\n",
        "          x = self.fc2(x)\n",
        "          output = F.log_softmax(x, dim=1)\n",
        "        else:\n",
        "          pass\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net(multiScale=True, cat=False)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv8okWPsDze0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# net(torch.randn(20, 1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKIwl6b3RffA",
        "colab_type": "text"
      },
      "source": [
        "###Train, Test and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp690s02VZM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7a5b484-42df-4f43-ae7b-e3c502999192"
      },
      "source": [
        "train(net, lr=0.001, num_epochs=10)\n",
        "# train(net, lr=0.003, num_epochs=5)\n",
        "train(net, lr=0.0001, num_epochs=20)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [100/781], Loss: 5.7623\n",
            "Epoch [1/10], Step [200/781], Loss: 7.1643\n",
            "Epoch [1/10], Step [300/781], Loss: 2.5556\n",
            "Epoch [1/10], Step [400/781], Loss: 1.7007\n",
            "Epoch [1/10], Step [500/781], Loss: 2.0281\n",
            "Epoch [1/10], Step [600/781], Loss: 1.6484\n",
            "Epoch [1/10], Step [700/781], Loss: 1.4830\n",
            "Epoch [2/10], Step [100/781], Loss: 1.6040\n",
            "Epoch [2/10], Step [200/781], Loss: 2.0097\n",
            "Epoch [2/10], Step [300/781], Loss: 1.3310\n",
            "Epoch [2/10], Step [400/781], Loss: 1.5470\n",
            "Epoch [2/10], Step [500/781], Loss: 1.4550\n",
            "Epoch [2/10], Step [600/781], Loss: 1.4153\n",
            "Epoch [2/10], Step [700/781], Loss: 1.4469\n",
            "Epoch [3/10], Step [100/781], Loss: 1.1666\n",
            "Epoch [3/10], Step [200/781], Loss: 1.2905\n",
            "Epoch [3/10], Step [300/781], Loss: 1.3657\n",
            "Epoch [3/10], Step [400/781], Loss: 1.3757\n",
            "Epoch [3/10], Step [500/781], Loss: 1.2555\n",
            "Epoch [3/10], Step [600/781], Loss: 1.3657\n",
            "Epoch [3/10], Step [700/781], Loss: 1.5478\n",
            "Epoch [4/10], Step [100/781], Loss: 1.3361\n",
            "Epoch [4/10], Step [200/781], Loss: 1.2477\n",
            "Epoch [4/10], Step [300/781], Loss: 1.3871\n",
            "Epoch [4/10], Step [400/781], Loss: 0.9881\n",
            "Epoch [4/10], Step [500/781], Loss: 1.3221\n",
            "Epoch [4/10], Step [600/781], Loss: 1.4547\n",
            "Epoch [4/10], Step [700/781], Loss: 1.2022\n",
            "Epoch [5/10], Step [100/781], Loss: 1.2959\n",
            "Epoch [5/10], Step [200/781], Loss: 1.5595\n",
            "Epoch [5/10], Step [300/781], Loss: 1.2274\n",
            "Epoch [5/10], Step [400/781], Loss: 0.9936\n",
            "Epoch [5/10], Step [500/781], Loss: 1.4271\n",
            "Epoch [5/10], Step [600/781], Loss: 1.2274\n",
            "Epoch [5/10], Step [700/781], Loss: 1.2023\n",
            "Epoch [6/10], Step [100/781], Loss: 1.2500\n",
            "Epoch [6/10], Step [200/781], Loss: 1.2989\n",
            "Epoch [6/10], Step [300/781], Loss: 1.4898\n",
            "Epoch [6/10], Step [400/781], Loss: 1.0644\n",
            "Epoch [6/10], Step [500/781], Loss: 0.8903\n",
            "Epoch [6/10], Step [600/781], Loss: 1.1365\n",
            "Epoch [6/10], Step [700/781], Loss: 1.3389\n",
            "Epoch [7/10], Step [100/781], Loss: 1.0328\n",
            "Epoch [7/10], Step [200/781], Loss: 1.0071\n",
            "Epoch [7/10], Step [300/781], Loss: 1.1566\n",
            "Epoch [7/10], Step [400/781], Loss: 0.9971\n",
            "Epoch [7/10], Step [500/781], Loss: 1.0024\n",
            "Epoch [7/10], Step [600/781], Loss: 1.4074\n",
            "Epoch [7/10], Step [700/781], Loss: 1.1168\n",
            "Epoch [8/10], Step [100/781], Loss: 1.2663\n",
            "Epoch [8/10], Step [200/781], Loss: 1.3220\n",
            "Epoch [8/10], Step [300/781], Loss: 0.9795\n",
            "Epoch [8/10], Step [400/781], Loss: 0.8844\n",
            "Epoch [8/10], Step [500/781], Loss: 0.7428\n",
            "Epoch [8/10], Step [600/781], Loss: 0.8994\n",
            "Epoch [8/10], Step [700/781], Loss: 1.4436\n",
            "Epoch [9/10], Step [100/781], Loss: 0.6774\n",
            "Epoch [9/10], Step [200/781], Loss: 0.9753\n",
            "Epoch [9/10], Step [300/781], Loss: 0.6045\n",
            "Epoch [9/10], Step [400/781], Loss: 1.0707\n",
            "Epoch [9/10], Step [500/781], Loss: 0.9690\n",
            "Epoch [9/10], Step [600/781], Loss: 1.1942\n",
            "Epoch [9/10], Step [700/781], Loss: 1.1579\n",
            "Epoch [10/10], Step [100/781], Loss: 0.8975\n",
            "Epoch [10/10], Step [200/781], Loss: 1.3127\n",
            "Epoch [10/10], Step [300/781], Loss: 0.8364\n",
            "Epoch [10/10], Step [400/781], Loss: 0.9267\n",
            "Epoch [10/10], Step [500/781], Loss: 0.9083\n",
            "Epoch [10/10], Step [600/781], Loss: 0.9607\n",
            "Epoch [10/10], Step [700/781], Loss: 0.6979\n",
            "Epoch [1/20], Step [100/781], Loss: 0.8478\n",
            "Epoch [1/20], Step [200/781], Loss: 0.5827\n",
            "Epoch [1/20], Step [300/781], Loss: 0.7479\n",
            "Epoch [1/20], Step [400/781], Loss: 0.6638\n",
            "Epoch [1/20], Step [500/781], Loss: 0.5986\n",
            "Epoch [1/20], Step [600/781], Loss: 0.5886\n",
            "Epoch [1/20], Step [700/781], Loss: 0.6034\n",
            "Epoch [2/20], Step [100/781], Loss: 0.7326\n",
            "Epoch [2/20], Step [200/781], Loss: 0.6555\n",
            "Epoch [2/20], Step [300/781], Loss: 0.8848\n",
            "Epoch [2/20], Step [400/781], Loss: 0.5163\n",
            "Epoch [2/20], Step [500/781], Loss: 0.7846\n",
            "Epoch [2/20], Step [600/781], Loss: 0.6905\n",
            "Epoch [2/20], Step [700/781], Loss: 0.6445\n",
            "Epoch [3/20], Step [100/781], Loss: 0.5491\n",
            "Epoch [3/20], Step [200/781], Loss: 0.7580\n",
            "Epoch [3/20], Step [300/781], Loss: 0.4661\n",
            "Epoch [3/20], Step [400/781], Loss: 0.4798\n",
            "Epoch [3/20], Step [500/781], Loss: 0.5812\n",
            "Epoch [3/20], Step [600/781], Loss: 0.7564\n",
            "Epoch [3/20], Step [700/781], Loss: 0.9022\n",
            "Epoch [4/20], Step [100/781], Loss: 0.5237\n",
            "Epoch [4/20], Step [200/781], Loss: 0.6545\n",
            "Epoch [4/20], Step [300/781], Loss: 0.6824\n",
            "Epoch [4/20], Step [400/781], Loss: 0.5637\n",
            "Epoch [4/20], Step [500/781], Loss: 0.7971\n",
            "Epoch [4/20], Step [600/781], Loss: 0.6545\n",
            "Epoch [4/20], Step [700/781], Loss: 0.4996\n",
            "Epoch [5/20], Step [100/781], Loss: 0.4935\n",
            "Epoch [5/20], Step [200/781], Loss: 0.6296\n",
            "Epoch [5/20], Step [300/781], Loss: 0.4545\n",
            "Epoch [5/20], Step [400/781], Loss: 0.6426\n",
            "Epoch [5/20], Step [500/781], Loss: 0.4766\n",
            "Epoch [5/20], Step [600/781], Loss: 0.7356\n",
            "Epoch [5/20], Step [700/781], Loss: 0.6199\n",
            "Epoch [6/20], Step [100/781], Loss: 0.2985\n",
            "Epoch [6/20], Step [200/781], Loss: 0.3882\n",
            "Epoch [6/20], Step [300/781], Loss: 0.4592\n",
            "Epoch [6/20], Step [400/781], Loss: 0.4705\n",
            "Epoch [6/20], Step [500/781], Loss: 0.3953\n",
            "Epoch [6/20], Step [600/781], Loss: 0.6628\n",
            "Epoch [6/20], Step [700/781], Loss: 0.4859\n",
            "Epoch [7/20], Step [100/781], Loss: 0.6386\n",
            "Epoch [7/20], Step [200/781], Loss: 0.5059\n",
            "Epoch [7/20], Step [300/781], Loss: 0.3261\n",
            "Epoch [7/20], Step [400/781], Loss: 0.3434\n",
            "Epoch [7/20], Step [500/781], Loss: 0.8432\n",
            "Epoch [7/20], Step [600/781], Loss: 0.7011\n",
            "Epoch [7/20], Step [700/781], Loss: 0.5919\n",
            "Epoch [8/20], Step [100/781], Loss: 0.5854\n",
            "Epoch [8/20], Step [200/781], Loss: 0.4532\n",
            "Epoch [8/20], Step [300/781], Loss: 0.6393\n",
            "Epoch [8/20], Step [400/781], Loss: 0.4026\n",
            "Epoch [8/20], Step [500/781], Loss: 0.4127\n",
            "Epoch [8/20], Step [600/781], Loss: 0.5515\n",
            "Epoch [8/20], Step [700/781], Loss: 0.7249\n",
            "Epoch [9/20], Step [100/781], Loss: 0.4029\n",
            "Epoch [9/20], Step [200/781], Loss: 0.4270\n",
            "Epoch [9/20], Step [300/781], Loss: 0.4500\n",
            "Epoch [9/20], Step [400/781], Loss: 0.3447\n",
            "Epoch [9/20], Step [500/781], Loss: 0.5642\n",
            "Epoch [9/20], Step [600/781], Loss: 0.5047\n",
            "Epoch [9/20], Step [700/781], Loss: 0.4763\n",
            "Epoch [10/20], Step [100/781], Loss: 0.4250\n",
            "Epoch [10/20], Step [200/781], Loss: 0.4025\n",
            "Epoch [10/20], Step [300/781], Loss: 0.6011\n",
            "Epoch [10/20], Step [400/781], Loss: 0.5131\n",
            "Epoch [10/20], Step [500/781], Loss: 0.5531\n",
            "Epoch [10/20], Step [600/781], Loss: 0.5908\n",
            "Epoch [10/20], Step [700/781], Loss: 0.3441\n",
            "Epoch [11/20], Step [100/781], Loss: 0.3576\n",
            "Epoch [11/20], Step [200/781], Loss: 0.4319\n",
            "Epoch [11/20], Step [300/781], Loss: 0.3747\n",
            "Epoch [11/20], Step [400/781], Loss: 0.4065\n",
            "Epoch [11/20], Step [500/781], Loss: 0.3513\n",
            "Epoch [11/20], Step [600/781], Loss: 0.2599\n",
            "Epoch [11/20], Step [700/781], Loss: 0.4989\n",
            "Epoch [12/20], Step [100/781], Loss: 0.3598\n",
            "Epoch [12/20], Step [200/781], Loss: 0.4436\n",
            "Epoch [12/20], Step [300/781], Loss: 0.2478\n",
            "Epoch [12/20], Step [400/781], Loss: 0.3956\n",
            "Epoch [12/20], Step [500/781], Loss: 0.2076\n",
            "Epoch [12/20], Step [600/781], Loss: 0.2438\n",
            "Epoch [12/20], Step [700/781], Loss: 0.3894\n",
            "Epoch [13/20], Step [100/781], Loss: 0.3124\n",
            "Epoch [13/20], Step [200/781], Loss: 0.3438\n",
            "Epoch [13/20], Step [300/781], Loss: 0.3049\n",
            "Epoch [13/20], Step [400/781], Loss: 0.4249\n",
            "Epoch [13/20], Step [500/781], Loss: 0.2631\n",
            "Epoch [13/20], Step [600/781], Loss: 0.2763\n",
            "Epoch [13/20], Step [700/781], Loss: 0.5352\n",
            "Epoch [14/20], Step [100/781], Loss: 0.2944\n",
            "Epoch [14/20], Step [200/781], Loss: 0.1223\n",
            "Epoch [14/20], Step [300/781], Loss: 0.4160\n",
            "Epoch [14/20], Step [400/781], Loss: 0.4076\n",
            "Epoch [14/20], Step [500/781], Loss: 0.2359\n",
            "Epoch [14/20], Step [600/781], Loss: 0.2367\n",
            "Epoch [14/20], Step [700/781], Loss: 0.1847\n",
            "Epoch [15/20], Step [100/781], Loss: 0.2688\n",
            "Epoch [15/20], Step [200/781], Loss: 0.2033\n",
            "Epoch [15/20], Step [300/781], Loss: 0.3985\n",
            "Epoch [15/20], Step [400/781], Loss: 0.3879\n",
            "Epoch [15/20], Step [500/781], Loss: 0.1547\n",
            "Epoch [15/20], Step [600/781], Loss: 0.3148\n",
            "Epoch [15/20], Step [700/781], Loss: 0.4398\n",
            "Epoch [16/20], Step [100/781], Loss: 0.4808\n",
            "Epoch [16/20], Step [200/781], Loss: 0.1894\n",
            "Epoch [16/20], Step [300/781], Loss: 0.1856\n",
            "Epoch [16/20], Step [400/781], Loss: 0.1962\n",
            "Epoch [16/20], Step [500/781], Loss: 0.2887\n",
            "Epoch [16/20], Step [600/781], Loss: 0.3686\n",
            "Epoch [16/20], Step [700/781], Loss: 0.2944\n",
            "Epoch [17/20], Step [100/781], Loss: 0.2063\n",
            "Epoch [17/20], Step [200/781], Loss: 0.3036\n",
            "Epoch [17/20], Step [300/781], Loss: 0.2878\n",
            "Epoch [17/20], Step [400/781], Loss: 0.1739\n",
            "Epoch [17/20], Step [500/781], Loss: 0.2641\n",
            "Epoch [17/20], Step [600/781], Loss: 0.2779\n",
            "Epoch [17/20], Step [700/781], Loss: 0.1542\n",
            "Epoch [18/20], Step [100/781], Loss: 0.4084\n",
            "Epoch [18/20], Step [200/781], Loss: 0.3581\n",
            "Epoch [18/20], Step [300/781], Loss: 0.3234\n",
            "Epoch [18/20], Step [400/781], Loss: 0.3233\n",
            "Epoch [18/20], Step [500/781], Loss: 0.1828\n",
            "Epoch [18/20], Step [600/781], Loss: 0.1827\n",
            "Epoch [18/20], Step [700/781], Loss: 0.1638\n",
            "Epoch [19/20], Step [100/781], Loss: 0.1693\n",
            "Epoch [19/20], Step [200/781], Loss: 0.3345\n",
            "Epoch [19/20], Step [300/781], Loss: 0.1664\n",
            "Epoch [19/20], Step [400/781], Loss: 0.1538\n",
            "Epoch [19/20], Step [500/781], Loss: 0.2277\n",
            "Epoch [19/20], Step [600/781], Loss: 0.1793\n",
            "Epoch [19/20], Step [700/781], Loss: 0.2763\n",
            "Epoch [20/20], Step [100/781], Loss: 0.2163\n",
            "Epoch [20/20], Step [200/781], Loss: 0.3776\n",
            "Epoch [20/20], Step [300/781], Loss: 0.2951\n",
            "Epoch [20/20], Step [400/781], Loss: 0.3047\n",
            "Epoch [20/20], Step [500/781], Loss: 0.3191\n",
            "Epoch [20/20], Step [600/781], Loss: 0.2186\n",
            "Epoch [20/20], Step [700/781], Loss: 0.2258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4ZFNmftV5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b8579476-bfe3-4bf7-bf50-ff1b84d1fcbd"
      },
      "source": [
        "test(net)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 95.422 %\n",
            "tensor(47712, device='cuda:0') 50000\n",
            "Test accuracy of the model: 46.955 %\n",
            "tensor(4696, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}