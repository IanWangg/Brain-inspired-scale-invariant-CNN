{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain-Inspired-Scale-Invariant-CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanWangg/Brain-inspired-scale-invariant-CNN/blob/weightsharing-Inception-convlayer/Brain_Inspired_Scale_Invariant_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJzF5aNOTdW",
        "colab_type": "text"
      },
      "source": [
        "## Code for implementing scale invariant CNNs.\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.160 %\n",
        "tensor(59497, device='cuda:0') 60000\n",
        "Test accuracy of the model: 82.702 %\n",
        "tensor(8271, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "Here are the results of scale invariance CNNs applied on MNIST. The train images are taken from MNIST directly where as test images are scaled by a factor sampled uniformly from [0.5,2.5].\n",
        "\n",
        "```\n",
        "RESULTS OF MULTISCALE CNN (bilinear)\n",
        "Train accuracy of the model: 99.210 %\n",
        "tensor(59527, device='cuda:0') 60000\n",
        "Test accuracy of the model: 72.533 %\n",
        "tensor(7254, device='cuda:0') 10000\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di4TD6kURjqA",
        "colab_type": "text"
      },
      "source": [
        "### Importing required libraries and setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21j-2kB6JyD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c526eea2-3b32-44d1-a5a4-893673493121"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.5.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndlXr-e4Rpjh",
        "colab_type": "text"
      },
      "source": [
        "### Rewriting Conv2d to implement the scale invariant convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw64jcc9SnNg",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the base class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbM5lGSOQOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.modules.utils import _single, _pair, _triple\n",
        "from torch.nn.modules.conv import *\n",
        "\n",
        "def _reverse_repeat_tuple(t, n):\n",
        "    \"\"\"Reverse the order of `t` and repeat each element for `n` times.\n",
        "    This can be used to translate padding arg used by Conv and Pooling modules\n",
        "    to the ones used by `F.pad`.\n",
        "    \"\"\"\n",
        "    return tuple(x for x in reversed(t) for _ in range(n))\n",
        "\n",
        "class _ConvNd(Module):\n",
        "\n",
        "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
        "                     'padding_mode', 'output_padding', 'in_channels',\n",
        "                     'out_channels', 'kernel_size']\n",
        "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, transposed, output_padding,\n",
        "                 groups, bias, padding_mode):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
        "        if padding_mode not in valid_padding_modes:\n",
        "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
        "                valid_padding_modes, padding_mode))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
        "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
        "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
        "        # reverse order than the dimension.\n",
        "        self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        if self.padding_mode != 'zeros':\n",
        "            s += ', padding_mode={padding_mode}'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(_ConvNd, self).__setstate__(state)\n",
        "        if not hasattr(self, 'padding_mode'):\n",
        "            self.padding_mode = 'zeros'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0WYIq5PSpYY",
        "colab_type": "text"
      },
      "source": [
        "#### Writing out new convolutional filter. Same number of parameters but convolutions at multiple scales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNcy4bSNRm0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "import random\n",
        "class Conv2dMultiScale(_ConvNd):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1,\n",
        "                 bias=True, padding_mode='zeros', num_scales=2, pooling_mode='max', level = 1):\n",
        "        cur_size = kernel_size\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(Conv2dMultiScale, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias, padding_mode)\n",
        "        # self.scale = nn.UpsamplingBilinear2d(size=(8,8))\n",
        "        self.scale = []\n",
        "        for i in range(num_scales - 1):\n",
        "          self.scale.append(nn.UpsamplingBilinear2d(size=(cur_size + 2, cur_size + 2)))\n",
        "        # self.scale = nn.functional.interpolate(size=(5,5), mode='bicubic')\n",
        "        self.pooling_mode = pooling_mode\n",
        "\n",
        "    def _conv_forward(self, input, weight_para):\n",
        "        out1 = F.conv2d(input, weight_para, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "        for scale in self.scale:          \n",
        "          weight = scale(weight_para)\n",
        "          padding = tuple(x+1 for x in self.padding)\n",
        "          out2 = F.conv2d(input, weight, self.bias, self.stride,\n",
        "                        padding, self.dilation, self.groups)\n",
        "          out2 = F.interpolate(out2, size=out1.shape[2:], mode='bilinear', align_corners=False)         \n",
        "          if (self.pooling_mode == 'avg'):\n",
        "            out1 = (out1 + out2)\n",
        "          elif (self.pooling_mode == 'max'):\n",
        "            out1 = torch.max(out1, out2)\n",
        "          else:\n",
        "            out1 = torch.cat([out1, out2], dim=1)\n",
        "        if (self.pooling_mode == 'cat'):\n",
        "          max_pool_results = F.max_pool2d(input, 2)\n",
        "          max_pool_results = F.interpolate(max_pool_results, size=out1.shape[2:], mode='bilinear', align_corners=False)         \n",
        "          out1 = torch.cat([out1, max_pool_results], dim=1)\n",
        "          # print(out1.shape, \"--------------------------------------------------------------------\")\n",
        "        return out1\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(input, self.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7NbCG3hpTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a23aae0-c9de-4207-a0d5-a29405018b34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpYqRAnM8Aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f4f98421-9e0a-4a3c-bedb-3f481d3b94a9"
      },
      "source": [
        "# Understanding sizes of CNNs, ensuring it works\n",
        "kernelSize = 5\n",
        "m = Conv2dMultiScale(3, 64, kernelSize, stride=1, padding=0, num_scales=2, pooling_mode='max')\n",
        "input = torch.randn(20, 3, 28, 28)\n",
        "print(input.shape)\n",
        "output = m(input)\n",
        "print(output.shape)\n",
        "output = F.max_pool2d(output, 2)\n",
        "print(output.shape)\n",
        "m2 = Conv2dMultiScale(64, 64, kernelSize, stride=1, padding=0, num_scales=3)\n",
        "output = m2(output)\n",
        "print(output.shape)\n",
        "output = F.max_pool2d(output, 2)\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 3, 28, 28])\n",
            "torch.Size([20, 64, 24, 24])\n",
            "torch.Size([20, 64, 12, 12])\n",
            "torch.Size([20, 64, 8, 8])\n",
            "torch.Size([20, 64, 4, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO3Ks9GN1Tef",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emawq_CRVXu",
        "colab_type": "text"
      },
      "source": [
        "###MNIST Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIJal8URUUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "273c6052-fcf8-4a46-a6ae-0c6a8ff4bf0d"
      },
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.RandomAffine(degrees=0, scale=(1,1)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "train_data = dsets.CIFAR10(root = './data', train = True,\n",
        "                        transform = train_transform, download = True)\n",
        "\n",
        "# test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "#                        transform = test_transform)\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "# test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "#                                       batch_size = batch_size, \n",
        "#                                       shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sls9YItQaOm",
        "colab_type": "text"
      },
      "source": [
        "###Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJAYjO4XQf5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, lr=0.001, num_epochs=10, batch_size=64):  \n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i ,(images,labels) in enumerate(train_gen):\n",
        "      if torch.cuda.is_available():\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(images)\n",
        "      loss = loss_function(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (i+1) % 100 == 0:\n",
        "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "def test(net):  \n",
        "  if(net.multiScale):\n",
        "    print('RESULTS OF MULTISCALE CNN')\n",
        "  else:\n",
        "    print('RESULTS OF STANDARD CNN')\n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # loss_function = nn.CrossEntropyLoss()\n",
        "  for images,labels in train_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  train_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Train accuracy of the model: %.3f %%' %(train_acc))\n",
        "  print(correct, total)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images,labels in test_gen:\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    output = net(images)\n",
        "    # loss = loss_function(outputs, labels)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "  test_acc = (100*correct.cpu().numpy())/(total+1)\n",
        "  print('Test accuracy of the model: %.3f %%' %(test_acc))\n",
        "  print(correct, total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfdjVqx8bZ_J",
        "colab_type": "text"
      },
      "source": [
        "###Define the Laplacian of Gaussian Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Iw4CeHbf8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import scipy\n",
        "\n",
        "def calculate(x, y, sigma):\n",
        "  return ((x**2 + y**2 - 2 * sigma**2) / sigma**4) * math.exp(-1 * (x**2 + y **2) / 2 * sigma**2)\n",
        "\n",
        "def create_log_filter(size, sigma):\n",
        "  matrix = [[0] * size for i in range(size)]\n",
        "  for i in range(size):\n",
        "    for j in range(size):\n",
        "      matrix[i][j] = calculate(i - size // 2, j - size // 2, sigma)\n",
        "  matrix = torch.Tensor(matrix)\n",
        "  # print(matrix)\n",
        "  return matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhSgsk9CRcW6",
        "colab_type": "text"
      },
      "source": [
        "###Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WykbLSM74b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_filter1 = create_log_filter(3, 2)\n",
        "log_filter2 = create_log_filter(3, 1)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, multiScale=True, cat = False, level_1=3, level_2=2):\n",
        "        super(Net, self).__init__()\n",
        "        self.multiScale = multiScale\n",
        "        self.cat = cat\n",
        "        self.level_1 = level_1\n",
        "        self.level_2 = level_2\n",
        "        if(self.multiScale and not cat):\n",
        "          self.conv1 = Conv2dMultiScale(3, 64, 5, 1, pooling_mode='max', num_scales=level_1)\n",
        "          self.conv2 = Conv2dMultiScale(64, 64, 5, 1, pooling_mode='max', num_scales=level_2)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        elif(cat and self.multiScale):\n",
        "          self.conv1 = Conv2dMultiScale(3, 64, 5, 1, pooling_mode='cat', num_scales=level_1)\n",
        "          self.conv2 = Conv2dMultiScale(64 * level_1 + 3, 64 * level_1 + 3, 5, 1, pooling_mode='cat', num_scales=level_2)\n",
        "          self.fc = nn.Linear((64 * level_1 + 3) * 5 * 5 * (level_2 + 1), 384)\n",
        "        else:\n",
        "          self.conv1 = nn.Conv2d(3, 64, 5, 1)\n",
        "          self.conv2 = nn.Conv2d(64, 64, 5, 1)\n",
        "          self.fc = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc1 = nn.Linear(384, 192)\n",
        "        self.fc2 = nn.Linear(192, 10)\n",
        "        self.log1 = nn.Conv2d(3, 3, 3, 1, padding=1)\n",
        "        self.log2 = nn.Conv2d(self.conv2.weight.shape[0], self.conv2.weight.shape[0], 3, 1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "          self.log1.weight[:][:] = log_filter1\n",
        "          self.log2.weight[:][:] = log_filter2\n",
        "        x = self.log1(x)\n",
        "        x = self.conv1(x)        \n",
        "        x = F.relu(x)    \n",
        "        x = F.max_pool2d(x, 2)  \n",
        "        # x = self.log2(x)\n",
        "        x = self.conv2(x)       \n",
        "        x = F.relu(x)      \n",
        "        x = F.max_pool2d(x, 2)     \n",
        "        x = torch.flatten(x, 1)       \n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)      \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        # else: \n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net(multiScale=True, cat=True)\n",
        "# print(net.conv2.weight.shape[0])\n",
        "# x = torch.randn(10, 3, 32, 32)\n",
        "# x = net.log1(x)\n",
        "# x.shape\n",
        "# with torch.no_grad():\n",
        "#     net.conv1.weight[:][:] = x\n",
        "# net.conv1.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKIwl6b3RffA",
        "colab_type": "text"
      },
      "source": [
        "###Train, Test and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp690s02VZM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba51b322-538f-4efb-d24c-cedb15301eee"
      },
      "source": [
        "train(net, lr=0.0005, num_epochs=5)\n",
        "train(net, lr=0.001, num_epochs=20)\n",
        "train(net, lr=0.0003, num_epochs=10)\n",
        "# train(net, lr=0.001, num_epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/781], Loss: 1.9251\n",
            "Epoch [1/5], Step [200/781], Loss: 2.1035\n",
            "Epoch [1/5], Step [300/781], Loss: 1.6747\n",
            "Epoch [1/5], Step [400/781], Loss: 1.7409\n",
            "Epoch [1/5], Step [500/781], Loss: 1.6707\n",
            "Epoch [1/5], Step [600/781], Loss: 1.5994\n",
            "Epoch [1/5], Step [700/781], Loss: 1.3192\n",
            "Epoch [2/5], Step [100/781], Loss: 1.3011\n",
            "Epoch [2/5], Step [200/781], Loss: 1.5204\n",
            "Epoch [2/5], Step [300/781], Loss: 1.3033\n",
            "Epoch [2/5], Step [400/781], Loss: 1.2429\n",
            "Epoch [2/5], Step [500/781], Loss: 1.4838\n",
            "Epoch [2/5], Step [600/781], Loss: 1.4141\n",
            "Epoch [2/5], Step [700/781], Loss: 1.3711\n",
            "Epoch [3/5], Step [100/781], Loss: 1.3832\n",
            "Epoch [3/5], Step [200/781], Loss: 1.6118\n",
            "Epoch [3/5], Step [300/781], Loss: 1.2625\n",
            "Epoch [3/5], Step [400/781], Loss: 1.1361\n",
            "Epoch [3/5], Step [500/781], Loss: 1.0781\n",
            "Epoch [3/5], Step [600/781], Loss: 1.3171\n",
            "Epoch [3/5], Step [700/781], Loss: 1.3587\n",
            "Epoch [4/5], Step [100/781], Loss: 1.1766\n",
            "Epoch [4/5], Step [200/781], Loss: 1.2461\n",
            "Epoch [4/5], Step [300/781], Loss: 1.0554\n",
            "Epoch [4/5], Step [400/781], Loss: 1.2275\n",
            "Epoch [4/5], Step [500/781], Loss: 1.0857\n",
            "Epoch [4/5], Step [600/781], Loss: 1.3720\n",
            "Epoch [4/5], Step [700/781], Loss: 1.1550\n",
            "Epoch [5/5], Step [100/781], Loss: 1.0609\n",
            "Epoch [5/5], Step [200/781], Loss: 0.9806\n",
            "Epoch [5/5], Step [300/781], Loss: 0.8694\n",
            "Epoch [5/5], Step [400/781], Loss: 1.0556\n",
            "Epoch [5/5], Step [500/781], Loss: 1.1757\n",
            "Epoch [5/5], Step [600/781], Loss: 1.0787\n",
            "Epoch [5/5], Step [700/781], Loss: 1.1926\n",
            "Epoch [1/20], Step [100/781], Loss: 1.0633\n",
            "Epoch [1/20], Step [200/781], Loss: 1.0264\n",
            "Epoch [1/20], Step [300/781], Loss: 1.1658\n",
            "Epoch [1/20], Step [400/781], Loss: 1.0153\n",
            "Epoch [1/20], Step [500/781], Loss: 1.1723\n",
            "Epoch [1/20], Step [600/781], Loss: 1.3184\n",
            "Epoch [1/20], Step [700/781], Loss: 0.9350\n",
            "Epoch [2/20], Step [100/781], Loss: 1.2470\n",
            "Epoch [2/20], Step [200/781], Loss: 1.0093\n",
            "Epoch [2/20], Step [300/781], Loss: 0.9671\n",
            "Epoch [2/20], Step [400/781], Loss: 1.1297\n",
            "Epoch [2/20], Step [500/781], Loss: 0.8730\n",
            "Epoch [2/20], Step [600/781], Loss: 0.9582\n",
            "Epoch [2/20], Step [700/781], Loss: 0.9793\n",
            "Epoch [3/20], Step [100/781], Loss: 0.8065\n",
            "Epoch [3/20], Step [200/781], Loss: 0.8220\n",
            "Epoch [3/20], Step [300/781], Loss: 0.9226\n",
            "Epoch [3/20], Step [400/781], Loss: 0.8664\n",
            "Epoch [3/20], Step [500/781], Loss: 0.9239\n",
            "Epoch [3/20], Step [600/781], Loss: 0.7107\n",
            "Epoch [3/20], Step [700/781], Loss: 1.0563\n",
            "Epoch [4/20], Step [100/781], Loss: 0.6597\n",
            "Epoch [4/20], Step [200/781], Loss: 0.7231\n",
            "Epoch [4/20], Step [300/781], Loss: 0.8226\n",
            "Epoch [4/20], Step [400/781], Loss: 0.7043\n",
            "Epoch [4/20], Step [500/781], Loss: 0.8988\n",
            "Epoch [4/20], Step [600/781], Loss: 0.7953\n",
            "Epoch [4/20], Step [700/781], Loss: 0.7933\n",
            "Epoch [5/20], Step [100/781], Loss: 0.8499\n",
            "Epoch [5/20], Step [200/781], Loss: 0.7237\n",
            "Epoch [5/20], Step [300/781], Loss: 0.8251\n",
            "Epoch [5/20], Step [400/781], Loss: 0.9594\n",
            "Epoch [5/20], Step [500/781], Loss: 0.7558\n",
            "Epoch [5/20], Step [600/781], Loss: 0.6535\n",
            "Epoch [5/20], Step [700/781], Loss: 0.7518\n",
            "Epoch [6/20], Step [100/781], Loss: 0.7711\n",
            "Epoch [6/20], Step [200/781], Loss: 0.5435\n",
            "Epoch [6/20], Step [300/781], Loss: 0.6434\n",
            "Epoch [6/20], Step [400/781], Loss: 0.7452\n",
            "Epoch [6/20], Step [500/781], Loss: 0.6003\n",
            "Epoch [6/20], Step [600/781], Loss: 0.6695\n",
            "Epoch [6/20], Step [700/781], Loss: 0.5954\n",
            "Epoch [7/20], Step [100/781], Loss: 0.5791\n",
            "Epoch [7/20], Step [200/781], Loss: 0.5204\n",
            "Epoch [7/20], Step [300/781], Loss: 0.4530\n",
            "Epoch [7/20], Step [400/781], Loss: 0.7500\n",
            "Epoch [7/20], Step [500/781], Loss: 0.7505\n",
            "Epoch [7/20], Step [600/781], Loss: 0.7882\n",
            "Epoch [7/20], Step [700/781], Loss: 0.8280\n",
            "Epoch [8/20], Step [100/781], Loss: 0.4680\n",
            "Epoch [8/20], Step [200/781], Loss: 0.6549\n",
            "Epoch [8/20], Step [300/781], Loss: 0.6097\n",
            "Epoch [8/20], Step [400/781], Loss: 0.4897\n",
            "Epoch [8/20], Step [500/781], Loss: 0.5174\n",
            "Epoch [8/20], Step [600/781], Loss: 0.5037\n",
            "Epoch [8/20], Step [700/781], Loss: 0.7431\n",
            "Epoch [9/20], Step [100/781], Loss: 0.5214\n",
            "Epoch [9/20], Step [200/781], Loss: 0.5230\n",
            "Epoch [9/20], Step [300/781], Loss: 0.4619\n",
            "Epoch [9/20], Step [400/781], Loss: 0.5394\n",
            "Epoch [9/20], Step [500/781], Loss: 0.4957\n",
            "Epoch [9/20], Step [600/781], Loss: 0.6914\n",
            "Epoch [9/20], Step [700/781], Loss: 0.4921\n",
            "Epoch [10/20], Step [100/781], Loss: 0.3403\n",
            "Epoch [10/20], Step [200/781], Loss: 0.4765\n",
            "Epoch [10/20], Step [300/781], Loss: 0.3735\n",
            "Epoch [10/20], Step [400/781], Loss: 0.5068\n",
            "Epoch [10/20], Step [500/781], Loss: 0.2941\n",
            "Epoch [10/20], Step [600/781], Loss: 0.3557\n",
            "Epoch [10/20], Step [700/781], Loss: 0.5489\n",
            "Epoch [11/20], Step [100/781], Loss: 0.6135\n",
            "Epoch [11/20], Step [200/781], Loss: 0.6680\n",
            "Epoch [11/20], Step [300/781], Loss: 0.3326\n",
            "Epoch [11/20], Step [400/781], Loss: 0.1940\n",
            "Epoch [11/20], Step [500/781], Loss: 0.3000\n",
            "Epoch [11/20], Step [600/781], Loss: 0.4287\n",
            "Epoch [11/20], Step [700/781], Loss: 0.5559\n",
            "Epoch [12/20], Step [100/781], Loss: 0.3482\n",
            "Epoch [12/20], Step [200/781], Loss: 0.2647\n",
            "Epoch [12/20], Step [300/781], Loss: 0.3712\n",
            "Epoch [12/20], Step [400/781], Loss: 0.2690\n",
            "Epoch [12/20], Step [500/781], Loss: 0.5523\n",
            "Epoch [12/20], Step [600/781], Loss: 0.2946\n",
            "Epoch [12/20], Step [700/781], Loss: 0.5218\n",
            "Epoch [13/20], Step [100/781], Loss: 0.3230\n",
            "Epoch [13/20], Step [200/781], Loss: 0.2803\n",
            "Epoch [13/20], Step [300/781], Loss: 0.2819\n",
            "Epoch [13/20], Step [400/781], Loss: 0.2071\n",
            "Epoch [13/20], Step [500/781], Loss: 0.2715\n",
            "Epoch [13/20], Step [600/781], Loss: 0.3110\n",
            "Epoch [13/20], Step [700/781], Loss: 0.3494\n",
            "Epoch [14/20], Step [100/781], Loss: 0.2043\n",
            "Epoch [14/20], Step [200/781], Loss: 0.2777\n",
            "Epoch [14/20], Step [300/781], Loss: 0.2352\n",
            "Epoch [14/20], Step [400/781], Loss: 0.3664\n",
            "Epoch [14/20], Step [500/781], Loss: 0.2853\n",
            "Epoch [14/20], Step [600/781], Loss: 0.1944\n",
            "Epoch [14/20], Step [700/781], Loss: 0.4217\n",
            "Epoch [15/20], Step [100/781], Loss: 0.2649\n",
            "Epoch [15/20], Step [200/781], Loss: 0.2778\n",
            "Epoch [15/20], Step [300/781], Loss: 0.4042\n",
            "Epoch [15/20], Step [400/781], Loss: 0.2859\n",
            "Epoch [15/20], Step [500/781], Loss: 0.7316\n",
            "Epoch [15/20], Step [600/781], Loss: 0.2743\n",
            "Epoch [15/20], Step [700/781], Loss: 0.3795\n",
            "Epoch [16/20], Step [100/781], Loss: 0.2022\n",
            "Epoch [16/20], Step [200/781], Loss: 0.1474\n",
            "Epoch [16/20], Step [300/781], Loss: 0.1128\n",
            "Epoch [16/20], Step [400/781], Loss: 0.3386\n",
            "Epoch [16/20], Step [500/781], Loss: 0.1696\n",
            "Epoch [16/20], Step [600/781], Loss: 0.3862\n",
            "Epoch [16/20], Step [700/781], Loss: 0.1743\n",
            "Epoch [17/20], Step [100/781], Loss: 0.3164\n",
            "Epoch [17/20], Step [200/781], Loss: 0.0538\n",
            "Epoch [17/20], Step [300/781], Loss: 0.0901\n",
            "Epoch [17/20], Step [400/781], Loss: 0.3036\n",
            "Epoch [17/20], Step [500/781], Loss: 0.2041\n",
            "Epoch [17/20], Step [600/781], Loss: 0.1287\n",
            "Epoch [17/20], Step [700/781], Loss: 0.2317\n",
            "Epoch [18/20], Step [100/781], Loss: 0.1096\n",
            "Epoch [18/20], Step [200/781], Loss: 0.1246\n",
            "Epoch [18/20], Step [300/781], Loss: 0.1846\n",
            "Epoch [18/20], Step [400/781], Loss: 0.2443\n",
            "Epoch [18/20], Step [500/781], Loss: 0.2055\n",
            "Epoch [18/20], Step [600/781], Loss: 0.3734\n",
            "Epoch [18/20], Step [700/781], Loss: 0.2599\n",
            "Epoch [19/20], Step [100/781], Loss: 0.2807\n",
            "Epoch [19/20], Step [200/781], Loss: 0.1401\n",
            "Epoch [19/20], Step [300/781], Loss: 0.2671\n",
            "Epoch [19/20], Step [400/781], Loss: 0.2098\n",
            "Epoch [19/20], Step [500/781], Loss: 0.1980\n",
            "Epoch [19/20], Step [600/781], Loss: 0.1640\n",
            "Epoch [19/20], Step [700/781], Loss: 0.1768\n",
            "Epoch [20/20], Step [100/781], Loss: 0.1757\n",
            "Epoch [20/20], Step [200/781], Loss: 0.1859\n",
            "Epoch [20/20], Step [300/781], Loss: 0.0948\n",
            "Epoch [20/20], Step [400/781], Loss: 0.2718\n",
            "Epoch [20/20], Step [500/781], Loss: 0.1765\n",
            "Epoch [20/20], Step [600/781], Loss: 0.2089\n",
            "Epoch [20/20], Step [700/781], Loss: 0.2763\n",
            "Epoch [1/10], Step [100/781], Loss: 0.0513\n",
            "Epoch [1/10], Step [200/781], Loss: 0.1298\n",
            "Epoch [1/10], Step [300/781], Loss: 0.0870\n",
            "Epoch [1/10], Step [400/781], Loss: 0.0616\n",
            "Epoch [1/10], Step [500/781], Loss: 0.1433\n",
            "Epoch [1/10], Step [600/781], Loss: 0.0667\n",
            "Epoch [1/10], Step [700/781], Loss: 0.0417\n",
            "Epoch [2/10], Step [100/781], Loss: 0.0133\n",
            "Epoch [2/10], Step [200/781], Loss: 0.0066\n",
            "Epoch [2/10], Step [300/781], Loss: 0.0227\n",
            "Epoch [2/10], Step [400/781], Loss: 0.0361\n",
            "Epoch [2/10], Step [500/781], Loss: 0.0115\n",
            "Epoch [2/10], Step [600/781], Loss: 0.0071\n",
            "Epoch [2/10], Step [700/781], Loss: 0.0331\n",
            "Epoch [3/10], Step [100/781], Loss: 0.0016\n",
            "Epoch [3/10], Step [200/781], Loss: 0.0025\n",
            "Epoch [3/10], Step [300/781], Loss: 0.0044\n",
            "Epoch [3/10], Step [400/781], Loss: 0.0039\n",
            "Epoch [3/10], Step [500/781], Loss: 0.0108\n",
            "Epoch [3/10], Step [600/781], Loss: 0.0016\n",
            "Epoch [3/10], Step [700/781], Loss: 0.0262\n",
            "Epoch [4/10], Step [100/781], Loss: 0.0117\n",
            "Epoch [4/10], Step [200/781], Loss: 0.0065\n",
            "Epoch [4/10], Step [300/781], Loss: 0.0060\n",
            "Epoch [4/10], Step [400/781], Loss: 0.0131\n",
            "Epoch [4/10], Step [500/781], Loss: 0.0137\n",
            "Epoch [4/10], Step [600/781], Loss: 0.0037\n",
            "Epoch [4/10], Step [700/781], Loss: 0.0100\n",
            "Epoch [5/10], Step [100/781], Loss: 0.0308\n",
            "Epoch [5/10], Step [200/781], Loss: 0.0166\n",
            "Epoch [5/10], Step [300/781], Loss: 0.1064\n",
            "Epoch [5/10], Step [400/781], Loss: 0.0192\n",
            "Epoch [5/10], Step [500/781], Loss: 0.0130\n",
            "Epoch [5/10], Step [600/781], Loss: 0.0065\n",
            "Epoch [5/10], Step [700/781], Loss: 0.0304\n",
            "Epoch [6/10], Step [100/781], Loss: 0.0394\n",
            "Epoch [6/10], Step [200/781], Loss: 0.0018\n",
            "Epoch [6/10], Step [300/781], Loss: 0.0487\n",
            "Epoch [6/10], Step [400/781], Loss: 0.0184\n",
            "Epoch [6/10], Step [500/781], Loss: 0.0152\n",
            "Epoch [6/10], Step [600/781], Loss: 0.0032\n",
            "Epoch [6/10], Step [700/781], Loss: 0.0419\n",
            "Epoch [7/10], Step [100/781], Loss: 0.0046\n",
            "Epoch [7/10], Step [200/781], Loss: 0.0158\n",
            "Epoch [7/10], Step [300/781], Loss: 0.0032\n",
            "Epoch [7/10], Step [400/781], Loss: 0.0255\n",
            "Epoch [7/10], Step [500/781], Loss: 0.0028\n",
            "Epoch [7/10], Step [600/781], Loss: 0.0129\n",
            "Epoch [7/10], Step [700/781], Loss: 0.0079\n",
            "Epoch [8/10], Step [100/781], Loss: 0.0077\n",
            "Epoch [8/10], Step [200/781], Loss: 0.0008\n",
            "Epoch [8/10], Step [300/781], Loss: 0.0010\n",
            "Epoch [8/10], Step [400/781], Loss: 0.0045\n",
            "Epoch [8/10], Step [500/781], Loss: 0.0023\n",
            "Epoch [8/10], Step [600/781], Loss: 0.0038\n",
            "Epoch [8/10], Step [700/781], Loss: 0.0286\n",
            "Epoch [9/10], Step [100/781], Loss: 0.0266\n",
            "Epoch [9/10], Step [200/781], Loss: 0.0211\n",
            "Epoch [9/10], Step [300/781], Loss: 0.0194\n",
            "Epoch [9/10], Step [400/781], Loss: 0.0125\n",
            "Epoch [9/10], Step [500/781], Loss: 0.0051\n",
            "Epoch [9/10], Step [600/781], Loss: 0.0008\n",
            "Epoch [9/10], Step [700/781], Loss: 0.0019\n",
            "Epoch [10/10], Step [100/781], Loss: 0.0205\n",
            "Epoch [10/10], Step [200/781], Loss: 0.0018\n",
            "Epoch [10/10], Step [300/781], Loss: 0.0029\n",
            "Epoch [10/10], Step [400/781], Loss: 0.0070\n",
            "Epoch [10/10], Step [500/781], Loss: 0.0285\n",
            "Epoch [10/10], Step [600/781], Loss: 0.0158\n",
            "Epoch [10/10], Step [700/781], Loss: 0.0152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkmy6hdofGLt",
        "colab_type": "text"
      },
      "source": [
        "###Test on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtRE-fD3nNS7",
        "colab_type": "text"
      },
      "source": [
        "###32 -> 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--PsvCNFe9Xt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a808728e-5ae4-4b78-d385-7a56b6b87f86"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform, download=True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)\n",
        "print(\"Architecture:\\n\",\"num_scale:\",net.level_1,\",\",net.level_2,\"\\nnet.Multiscale=\",net.multiScale,\"\\ncat=\",net.cat)\n",
        "test(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 99.482 %\n",
            "tensor(49742, device='cuda:0') 50000\n",
            "Test accuracy of the model: 65.153 %\n",
            "tensor(6516, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNWAFnSonUOy",
        "colab_type": "text"
      },
      "source": [
        "###32 -> 28"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4ZFNmftV5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "94bb5a52-257e-4f9e-f98a-b1587c394bec"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(28),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform, download=True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)\n",
        "print(\"Architecture:\\n\",\"num_scale:\",net.level_1,\",\",net.level_2,\"\\nnet.Multiscale=\",net.multiScale,\"\\ncat=\",net.cat)\n",
        "test(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 99.482 %\n",
            "tensor(49742, device='cuda:0') 50000\n",
            "Test accuracy of the model: 57.194 %\n",
            "tensor(5720, device='cuda:0') 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRyjrSxKnWYI",
        "colab_type": "text"
      },
      "source": [
        "###32 -> 24"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iafy3nlNnIkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ba6f79f9-44e8-4112-ddd3-f64749751fea"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(24),\n",
        "    transforms.Resize(32, interpolation=2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_data = dsets.CIFAR10(root = './data', train = False,\n",
        "                       transform = test_transform, download=True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)\n",
        "print(\"Architecture:\\n\",\"num_scale:\",net.level_1,\",\",net.level_2,\"\\nnet.Multiscale=\",net.multiScale,\"\\ncat=\",net.cat)\n",
        "test(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n",
            "Train accuracy of the model: 99.482 %\n",
            "tensor(49742, device='cuda:0') 50000\n",
            "Test accuracy of the model: 45.915 %\n",
            "tensor(4592, device='cuda:0') 10000\n",
            "Files already downloaded and verified\n",
            "Architecture:\n",
            " num_scale: 3 , 2 \n",
            "net.Multiscale= True \n",
            "cat= False\n",
            "RESULTS OF MULTISCALE CNN\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}